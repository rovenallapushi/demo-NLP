{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "C:\\Users\\roven\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roven\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] \n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 7.98MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 9.70MB/s]\n"
     ]
    }
   ],
   "source": [
    "# load them both using from_pretrained function\n",
    "# the opposite function is the save_pretrained function, which saves the model and tokenizer to disk\n",
    "# the tokenizer is used to convert the text into tokens, which are then fed into the model\n",
    "# the model is the actual neural network that will be trained\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer applies the following step:\n",
    "1. Preprocesses the text and tokenizes it in subwords\n",
    "2. Associates to every subword an input_id with is used to fetch its embedding in the embedding layer\n",
    "3. Adds attention_mask and token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 12909, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I love Luna\", add_special_tokens=True)\n",
    "# ['[CLS]', 'i', 'love', 'luna', '[SEP]']\n",
    "tokenizer(\"I love Luna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'love', 'luna', '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some other functions that the tokenizer provides\n",
    "# convert tokens to ids and ids to tokens\n",
    "\n",
    "output = tokenizer(\"I love Luna\")\n",
    "tokenizer.convert_ids_to_tokens(output[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i love luna [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all sentences are reconstructed using the tokenizer.decode function\n",
    "\n",
    "output = tokenizer(\"I love Luna\")\n",
    "tokenizer.decode(output[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  2293, 12909,   102,     0],\n",
      "        [  101,  1045,  2293,  8879,   102,     0],\n",
      "        [  101,  1045,  2293, 21025, 21818,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# attention masks are used to tell the model which tokens to pay attention to and which to ignore\n",
    "# the attention mask is a binary tensor that is the same size as the tokenized input\n",
    "# the attention mask has a 1 for all the tokens that are not masked and a 0 for all the tokens that are masked\n",
    "sentences = [\"I love Luna\", \"I love Marco\", \"I love Giove\"]\n",
    "output = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(output[\"input_ids\"])\n",
    "print(output[\"attention_mask\"])\n",
    "\n",
    "# commenting the result of input_ids and attention_mask\n",
    "# tensor([[ 101, 1045, 2293, 5959,  102],\n",
    "#         [ 101, 1045, 2293, 6207,  102],\n",
    "#         [ 101, 1045, 2293,  102,    0]])\n",
    "# means pay attention to the words that are non zero, for the first sentence we add a PAD token at the end, that's why we have a 0 at the end of the first sentence\n",
    "# tensor([[1, 1, 1, 1, 1],\n",
    "#         [1, 1, 1, 1, 1],\n",
    "#         [1, 1, 1, 0, 0]])\n",
    "# means pay attention to all the words in the first two sentences, but only to the first three words in the third sentence\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input_ids \n",
    "\n",
    "Each subword is associated to a input_id which tells the model which embedding to get for that subword\n",
    "\n",
    "The attention_mask\n",
    "\n",
    "Suppose we have 2 sentences of different lengths\n",
    "if the sentences are in the same batch, the shortest one needs to be padded: we need to append [pad] tokens to the shortest sentence so that they have the same length\n",
    "Tokenizer handles all of this for us ( even to not pay attention ove the PAD tokens)\n",
    "\n",
    "The token_type_ids\n",
    "Input embeddings to a transformers are the result of a sum of three elements:\n",
    " 1. token embeddings: the embeddings that are extracted from the embedding matrix using input_ids\n",
    " 2. positional embeddings: this are sinusoidal or learned and give the tranformer the position information\n",
    " tells the tranformer in which position each subword is associated to.\n",
    " 3. Segment embeddings: when we are doing sentence-pair task\n",
    " when the input is made of 2 sentence pairs; by adding segment embedding we want to tell for every subword its originating sentence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] the sun is shining today [SEP] today it's rainy [SEP]\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer(\"The sun is shining today\",\"Today it's rainy\")\n",
    "output\n",
    "tokenizer.decode(output[\"input_ids\"])   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BertModel(\n",
    "  (embeddings): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "    (position_embeddings): Embedding(512, 768)\n",
    "    (token_type_embeddings): Embedding(2, 768)\n",
    "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (encoder): BertEncoder(\n",
    "    (layer): ModuleList(\n",
    "      (0-11): 12 x BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (pooler): BertPooler(\n",
    "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "    (activation): Tanh()\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding a batch to a tranformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2478, 25283, 14192,  2545,  2003,  3243,  3722,   102,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  3019, 11374, 16377,  3351,  6364,  2003,  1996,  4658,  4355,\n",
      "          2181,  1997,  9932,   102],\n",
      "        [  101, 14324,  2003,  2019,  4372, 16044,  2099,  1011,  2069,  2944,\n",
      "           102,     0,     0,     0]])\n",
      "\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])\n",
      "\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "\n",
      "['[CLS] using tranformers is quite simple [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] natural langugage processing is the coolest area of ai [SEP]', '[CLS] bert is an encoder - only model [SEP] [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "sequences = [\"Using tranformers is quite simple\", \"Natural Langugage Processing is the coolest area of AI\", \"BERT is an encoder-only model\"]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(batch[\"input_ids\"], batch[\"attention_mask\"],batch[\"token_type_ids\"], sep=\"\\n\\n\", end=\"\\n\\n\")\n",
    "print(tokenizer.batch_decode(batch[\"input_ids\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: given two sentences, assign positive class(1) if the two sentences are paraphrases of one another (assign 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 20.7MB/s]\n",
      "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 28.7MB/s]\n",
      "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 27.9MB/s]\n",
      "C:\\Users\\roven\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mrpc to C:/Users/roven/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 6.22kB [00:00, 3.10MB/s]/3 [00:00<?, ?it/s]\n",
      "Downloading data: 1.05MB [00:00, 20.8MB/s]/3 [00:00<00:00,  3.42it/s]\n",
      "Downloading data: 441kB [00:00, 16.3MB/s]2/3 [00:00<00:00,  3.79it/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  2.14it/s]\n",
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to C:/Users/roven/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 33.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "mrpc_dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "mrpc_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataset features and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n",
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
      "\n",
      "{'sentence1': \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\", 'sentence2': \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\", 'label': 0, 'idx': 1}\n"
     ]
    }
   ],
   "source": [
    "print(mrpc_dataset[\"train\"].features)\n",
    "print(mrpc_dataset[\"train\"][0], end = \"\\n\\n\")\n",
    "print(mrpc_dataset[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence1': ['This Palm OS smart phone is the last product the company will release before it becomes a part of palmOne .',\n",
       "  \"This week 's tour will take Bush to Senegal , South Africa , Botswana , Uganda and Nigeria , and is aimed at softening his warrior image at home and abroad .\",\n",
       "  'This was around the time Congress was debating a resolution granting the President broad authority to wage war .',\n",
       "  \"This morning , at UM 's New York office , Coen revised his expectations downward , saying that spending would instead rise 4.6 percent to $ 247 billion .\",\n",
       "  'This is the only planet that has been found in orbit around a binary star system .',\n",
       "  'This year , local health departments hired part-time water samplers and purchased testing equipment with a $ 282,355 grant from the Environmental Protection Agency .',\n",
       "  'This was double the $ 818 million reported for the first three months of 2001 .',\n",
       "  'This change in attitude gave upscale purveyors including Neiman Marcus , the parent of Bergdorf Goodman ; and Nordstrom strong sales gains in May .',\n",
       "  'This is a process and there will be other opportunities for people to participate in the rebuilding of Iraq . \" he told reporters .',\n",
       "  'This deterioration of security compounds when nearly all computers rely on a single operating system subject to the same vulnerabilities the world over , \" Geer added .',\n",
       "  'This means Berlusconi will be safe from prosecution until he leaves elected office , scheduled for 2006 .',\n",
       "  'This is a case about a woman who for 20 years dedicated her life to this country , \" said Janet Levine .',\n",
       "  'This is the first time in the United States that five whales have been released simultaneously from a single stranding incident .',\n",
       "  'This is what Dr. Dean said : \" I still want to be the candidate for guys with Confederate flags in their pickup trucks .',\n",
       "  'This decision would \" throw a monkey wrench into the FCC \\'s efforts to develop a vitally important national broadband policy , \" he said .'],\n",
       " 'sentence2': ['This was almost certainly its last full quarter before the company becomes a part of Palm .',\n",
       "  'In his first trip to sub-Saharan Africa as president , Mr. Bush will visit Senegal , South Africa , Botswana , Uganda and Nigeria before returning home on Saturday .',\n",
       "  'Within four days , the House and Senate overwhelmingly endorsed a resolution granting the president authority to go to war .',\n",
       "  \"Speaking to reporters at a New York news conference , Universal McCann 's Coen projected that total U.S. ad spending will rise 4.6 percent to $ 247.7 billion this year .\",\n",
       "  'The new found planet is the only one known to orbit such a double-star system .',\n",
       "  'This year , Peninsula health officials got the money to hire part-time water samplers and purchase testing equipment thanks to a $ 282,355 grant from the Environmental Protection Agency .',\n",
       "  'Berkshire Hathaway made profits of $ 1.7 billion in the first three months of this year alone , its best performance .',\n",
       "  'This change in attitude gave upscale purveyors including Neiman Marcus Group Inc. and Nordstrom Inc . , along with some boutique retailers , strong sales gains in May .',\n",
       "  'This is a process and there will be other opportunities for people to participate in the rebuilding of Iraq . \"',\n",
       "  '\" The deterioration of security compounds when nearly all computers rely on a single operating system subject to the same vulnerabilities the world over . \"',\n",
       "  'This means Berlusconi will be safe from prosecution until his term ends in 2006 , unless his government falls before then .',\n",
       "  '\" This case is about a woman who for over 20 years dedicated herself to this country , \" Levine said .',\n",
       "  \"Today , the experts will perform the United States ' first simultaneous release of five whales from a single stranding incident .\",\n",
       "  'He told the Register : \" I still want to be the candidate for guys with Confederate flags in their pickup trucks . \"',\n",
       "  'He added that the decision will \" throw a monkey wrench into the FCC \\'s efforts to develop a vitally important national broadband policy . \"'],\n",
       " 'label': [0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1],\n",
       " 'idx': [97,\n",
       "  482,\n",
       "  1025,\n",
       "  1431,\n",
       "  1450,\n",
       "  1478,\n",
       "  1569,\n",
       "  1783,\n",
       "  2229,\n",
       "  2319,\n",
       "  2444,\n",
       "  2567,\n",
       "  2724,\n",
       "  3247,\n",
       "  3439]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered rows that match a specific condition\n",
    "filtered_data = mrpc_dataset[\"train\"].filter(lambda example: example[\"sentence1\"].startswith(\"This\"))\n",
    "filtered_data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3301\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 367\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "\n",
    "mrpc_dataset[\"train\"].train_test_split(test_size=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
