{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roven\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcStandard:\n",
    "    def __init__(self, sentence):\n",
    "\n",
    "        # sentence is the input for which we want to build our Arc-Standard\n",
    "        self.sentence = sentence\n",
    "\n",
    "        # here we create the buffer having an array of indexes with the same length as the sentence\n",
    "        # basically, each word has its own index in this buffer\n",
    "        # we have initialized the buffer having all the words in the sentence\n",
    "\n",
    "        self.buffer = [i for i in range(len(self.sentence))] \n",
    "\n",
    "        # initialize the stack empty \n",
    "        \n",
    "        self.stack = []\n",
    "\n",
    "        # representation of the tree\n",
    "        # every word will have a -1 assigned -> no father has been assigned yet\n",
    "\n",
    "        self.arcs = [-1 for _ in range(len(self.sentence))]\n",
    "\n",
    "        # three shift moves to initialize the stack\n",
    "\n",
    "        # means that in the stack now is the ROOT\n",
    "        # self.shift() it calls a method that implements this operation; we will look at it after \n",
    "\n",
    "        self.shift() \n",
    "\n",
    "        # means that in the stack now is the ROOT and He; sigma1 is He and sigma2 is the ROOT\n",
    "\n",
    "        self.shift() \n",
    "\n",
    "        # until now we cannnot attach He to ROOT, if we cannot do that we add another element to the stack;\n",
    "        # so, perform another shift\n",
    "        # now, in the stack we have ROOT, He, began\n",
    "        if len(self.sentence) > 2:\n",
    "          self.shift()\n",
    "\n",
    "    def shift(self):\n",
    "       \n",
    "     b1 = self.buffer[0]\n",
    "     self.buffer = self.buffer[1:]\n",
    "     self.stack.append(b1)\n",
    "    \n",
    "    def left_arc(self): \n",
    "\n",
    "     o1 = self.stack.pop()\n",
    "     o2 = self.stack.pop()\n",
    "     self.arcs[o2] = o1\n",
    "     self.stack.append(o1)\n",
    "\n",
    "     if len(self.stack) < 2 and len(self.buffer) > 0:\n",
    "        self.shift()\n",
    "    \n",
    "\n",
    "    def right_arc(self):\n",
    "     o1 = self.stack.pop()\n",
    "     o2 = self.stack.pop()\n",
    "     self.arcs[o1] = o2\n",
    "     self.stack.append(o2)\n",
    "\n",
    "     if len(self.stack) <2 and len(self.buffer) > 0:\n",
    "        self.shift()\n",
    "    \n",
    "    def is_tree_final(self):\n",
    "     return len(self.stack) == 1 and len(self.buffer) == 0\n",
    "    \n",
    "\n",
    "    def print_configuration(self):\n",
    "\n",
    "      s = [self.sentence[i] for i in self.stack]\n",
    "      b = [self.sentence[i] for i in self.buffer]\n",
    "      print(s,b)\n",
    "      print(self.arcs)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ROOT>', 'He', 'began'] ['to', 'write', 'again', '.']\n",
      "[-1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"<ROOT>\", \"He\",\"began\",\"to\",\"write\",\"again\",\".\"]\n",
    "gold = [-1, 2, 0, 4, 2, 4, 2]\n",
    "\n",
    "parser = ArcStandard(sentence)\n",
    "parser.print_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ROOT>', 'began'] ['to', 'write', 'again', '.']\n",
      "[-1, 2, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "parser.left_arc()\n",
    "parser.print_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ROOT>', 'began', 'to'] ['write', 'again', '.']\n",
      "[-1, 2, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "parser.shift()\n",
    "parser.print_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ROOT>', 'began'] ['write', 'again', '.']\n",
      "[-1, 2, -1, 2, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "parser.right_arc()\n",
    "parser.print_configuration()\n",
    "\n",
    "# we are asking the parser to do this action of the right_arc; \"to\" has as a paretn \"write\" and \"began\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle:\n",
    " def __init__(self, parser, gold_tree):\n",
    "  self.parser = parser\n",
    "  self.gold = gold_tree\n",
    "\n",
    " def is_left_arc_gold(self):\n",
    "  \n",
    "  # we can do the left arc if sigma2 is the children of the sigma1\n",
    "  # this means that u have assigned to sigma2 all of its children and is ok to assing its parent -> static oracle\n",
    "  \n",
    "  # here we get our sigma1 and sigma2\n",
    "\n",
    "  o1 = self.parser.stack[len(self.parser.stack)-1]\n",
    "  o2 = self.parser.stack[len(self.parser.stack)-2]\n",
    "\n",
    "\n",
    "  if self.gold[o2] == o1:\n",
    "   return True\n",
    "  return False\n",
    "\n",
    " def is_shift_gold(self):\n",
    " \n",
    "  if len(self.parser.buffer) == 0:\n",
    "   return False\n",
    " \n",
    "  if (self.is_left_arc_gold() or self.is_right_arc_gold()):\n",
    "   return False\n",
    " \n",
    "  return True\n",
    "\n",
    " def is_right_arc_gold(self):\n",
    "   o1 = self.parser.stack[len(self.parser.stack)-1]\n",
    "   o2 = self.parser.stack[len(self.parser.stack)-2]\n",
    "\n",
    "   if self.gold[o1] != o2:\n",
    "    return False\n",
    "   \n",
    "   # we need to check that no children of sigma1 are in the rest of the buffer\n",
    "   \n",
    "   for i in self.parser.buffer:\n",
    "    if self.gold[i] == o1:\n",
    "     return False\n",
    "   return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ROOT>', 'He', 'began'] ['to', 'write', 'again', '.']\n",
      "[-1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"<ROOT>\", \"He\",\"began\",\"to\",\"write\",\"again\",\".\"]\n",
    "gold = [-1, 2, 0, 4, 2, 4, 2]\n",
    "\n",
    "parser = ArcStandard(sentence)\n",
    "oracle = Oracle(parser, gold)\n",
    "\n",
    "parser.print_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Arc:  True\n",
      "Right Arc:  False\n",
      "Shift:  False\n"
     ]
    }
   ],
   "source": [
    "print(\"Left Arc: \" ,oracle.is_left_arc_gold())\n",
    "print(\"Right Arc: \",oracle.is_right_arc_gold())\n",
    "print(\"Shift: \",oracle.is_shift_gold())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ROOT>', 'began'] ['to', 'write', 'again', '.']\n",
      "[-1, 2, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "# oracle tells us that hte next move is the left_arc wqe do it and ask again the Oracle\n",
    "parser.left_arc()\n",
    "parser.print_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left Arc:  False\n",
      "Right Arc:  False\n",
      "Shift:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Left Arc: \" ,oracle.is_left_arc_gold())\n",
    "print(\"Right Arc: \",oracle.is_right_arc_gold())\n",
    "print(\"Shift: \",oracle.is_shift_gold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 2, 0, 4, 2, 4, 2]\n",
      "[-1, 2, 0, 4, 2, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "while not parser.is_tree_final():\n",
    "    if oracle.is_shift_gold():\n",
    "        parser.shift()\n",
    "    elif oracle.is_left_arc_gold():\n",
    "        parser.left_arc()\n",
    "    elif oracle.is_right_arc_gold():\n",
    "        parser.right_arc()\n",
    "        \n",
    "print(parser.arcs)\n",
    "print(gold)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for now we have build the parser and Oracle, we need to use them to guide a neural network model. We will give the sentence to BiLSTM to obtain some contextual representation and then use a Feed Forward to score the parser configuration and what is the best transition to do as a next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roven\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Found cached dataset universal_dependencies (C:/Users/roven/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('universal_dependencies', 'en_lines', split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3176\n",
      "dict_keys(['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'])\n",
      "['About', 'ANSI', 'SQL', 'query', 'mode']\n",
      "['5', '5', '2', '5', '0']\n"
     ]
    }
   ],
   "source": [
    "# info about the length of dataset\n",
    "print(len(dataset))\n",
    "\n",
    "# what kind of information is stored in this dataset\n",
    "\n",
    "print(dataset[1].keys())\n",
    "\n",
    "# print a sentence and its tokens and gold dependency tree\n",
    "\n",
    "print(dataset[1][\"tokens\"])\n",
    "print(dataset[1][\"head\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns whether a tree is projective or not\n",
    "\n",
    "def is_projective(tree):\n",
    "    for i in range(len(tree)):\n",
    "        if tree[i]==-1:\n",
    "            continue\n",
    "        left = min(i, tree[i])\n",
    "        right = max(i, tree[i])\n",
    "\n",
    "        for j in range(0, left):\n",
    "            if tree[j] > left and tree[j] < right:\n",
    "                return False\n",
    "        for j in range(left+1, right):\n",
    "            if tree[j] < left or tree[j] > right:\n",
    "                return False\n",
    "        for j in range(right+1, len(tree)):\n",
    "            if tree[j] > left  and tree[j] < right:\n",
    "                return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(dataset, threshold = 3):\n",
    "    dic = {}\n",
    "\n",
    "    for sample in dataset:\n",
    "        for word in sample['tokens']:\n",
    "            if word in dic:\n",
    "                dic[word] += 1\n",
    "            else:\n",
    "                dic[word] = 1\n",
    "    \n",
    "    map = {}\n",
    "    map[\"<pad>\"] = 0\n",
    "    map[\"<ROOT>\"] = 1\n",
    "    map[\"<unk>\"] = 2\n",
    "\n",
    "    next_indx = 3\n",
    "    for word in dic.keys():\n",
    "        if dic[word] >= threshold:\n",
    "            map[word] = next_indx\n",
    "            next_indx += 1\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset universal_dependencies (C:/Users/roven/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n",
      "Found cached dataset universal_dependencies (C:/Users/roven/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n",
      "Found cached dataset universal_dependencies (C:/Users/roven/.cache/huggingface/datasets/universal_dependencies/en_lines/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:\n",
      "Train:\t 2922\n",
      "Dev:\t 1032\n",
      "Test:\t 1035\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('universal_dependencies', 'en_lines', split = 'train')\n",
    "dev_dataset = load_dataset('universal_dependencies', 'en_lines', split = 'validation')\n",
    "test_dataset = load_dataset('universal_dependencies', 'en_lines', split = 'test')\n",
    "\n",
    "# remove non-projective sentences: heads in the gold \n",
    "# tree are strings, we convert them to int\n",
    "\n",
    "train_dataset =[sample for sample in train_dataset if is_projective([-1] + [int(head) for head in sample['head']])]\n",
    "\n",
    "# create embedding dictionary\n",
    "\n",
    "emb_dictionary = create_dict(train_dataset)\n",
    "\n",
    "\n",
    "print(\"Number of samples:\")\n",
    "print(\"Train:\\t\", len(train_dataset))\n",
    "print(\"Dev:\\t\", len(dev_dataset))\n",
    "print(\"Test:\\t\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample, get_gold_path = False):\n",
    "\n",
    "  # put sentence and gold tree in our format\n",
    "  sentence = [\"<ROOT>\"] + sample[\"tokens\"]\n",
    "  gold = [-1] + [int(i) for i in sample[\"head\"]]  #heads in the gold tree are strings, we convert them to int\n",
    "  \n",
    "  # embedding ids of sentence words\n",
    "  enc_sentence = [emb_dictionary[word] if word in emb_dictionary else emb_dictionary[\"<unk>\"] for word in sentence]\n",
    "\n",
    "  # gold_path and gold_moves are parallel arrays whose elements refer to parsing steps\n",
    "  gold_path = []   # record two topmost stack tokens and first buffer token for current step\n",
    "  gold_moves = []  # contains oracle (canonical) move for current step: 0 is left, 1 right, 2 shift\n",
    "\n",
    "  if get_gold_path:  # only for training\n",
    "    parser = ArcStandard(sentence)\n",
    "    oracle = Oracle(parser, gold)\n",
    "\n",
    "    while not parser.is_tree_final():\n",
    "      \n",
    "      # save configuration\n",
    "      configuration = [parser.stack[len(parser.stack)-2], parser.stack[len(parser.stack)-1]]\n",
    "      if len(parser.buffer) == 0:\n",
    "        configuration.append(-1)\n",
    "      else:\n",
    "        configuration.append(parser.buffer[0])  \n",
    "      gold_path.append(configuration)\n",
    "\n",
    "      # save gold move\n",
    "      if oracle.is_left_arc_gold():  \n",
    "        gold_moves.append(0)\n",
    "        parser.left_arc()\n",
    "      elif oracle.is_right_arc_gold():\n",
    "        parser.right_arc()\n",
    "        gold_moves.append(1)\n",
    "      elif oracle.is_shift_gold():\n",
    "        parser.shift()\n",
    "        gold_moves.append(2)\n",
    "\n",
    "  return enc_sentence, gold_path, gold_moves, gold\n",
    "\n",
    "    # gold_path stores the configurations of the stack and the buffer\n",
    "    # gold_moves stores the correct gold move at each configuration\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(batch_data, get_gold_path = False):\n",
    "    data = [process_sample(s, get_gold_path = get_gold_path) for s in batch_data]\n",
    "\n",
    "    sentences = [s[0] for s in data]\n",
    "    paths = [s[1] for s in data]\n",
    "    moves = [s[2] for s in data]\n",
    "    trees = [s[3] for s in data]\n",
    "\n",
    "    return sentences, paths, moves, trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle = True, collate_fn = partial(prepare_batch, get_gold_path = True))\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size= BATCH_SIZE, shuffle = True, collate_fn = partial(prepare_batch))\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle = True, collate_fn = partial(prepare_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters of NN\n",
    "EMBEDDING_SIZE = 200\n",
    "LSTM_SIZE = 200\n",
    "LSTM_LAYERS = 1\n",
    "MLP_SIZE = 200\n",
    "DROPOUT = 0.2\n",
    "EPOCHS = 15\n",
    "LR = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(Net, self).__init__()\n",
    "        self.device = device\n",
    "        self.embeddings = nn.Embedding(len(emb_dictionary), EMBEDDING_SIZE, padding_idx = emb_dictionary[\"<pad>\"])\n",
    "\n",
    "        # initialize bi-LSTM\n",
    "\n",
    "        self.lstm = nn.LSTM(EMBEDDING_SIZE, LSTM_SIZE, num_layers = LSTM_LAYERS, bidirectional = True, dropout = DROPOUT)\n",
    "\n",
    "        # initialize feedforward\n",
    "\n",
    "        self.w1 = torch.nn.Linear(6*LSTM_SIZE, MLP_SIZE, bias=True)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "        self.w2 = torch.nn.Linear(MLP_SIZE, 3, bias = True)\n",
    "        self.softmax = torch.nn. Softmax(dim = -1)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "\n",
    "\n",
    "    def forward(self, x, paths):\n",
    "            # get the embeddings\n",
    "            # x - sentence, index of the embeddings (batch of sentences in this case)\n",
    "            # paths - parser configuration\n",
    "\n",
    "        x = [self.dropout(self.embeddings(torch.tensor(i).to(self.device))) for i in x]\n",
    "\n",
    "            # run bi-lstm\n",
    "            # gets as input the embeddings and then as output we will have some embeddings that are contextualized\n",
    "\n",
    "        h = self.lstm_pass(x)\n",
    "\n",
    "            # for each parser configuration that we need to score we arrange from the\n",
    "            # output of the bi - lstm the correct input for the feedforward\n",
    "            # we have the sentence in contextualized representation and we want to get all the parser configuration;\n",
    "            # 2 elemetns of the stack and 1 of the buffer, we will get these 3 embeddings, concatenate them together and then \n",
    "            #  pass them into feed forward (multilayer perceptron)\n",
    "\n",
    "        mlp_input = self.get_mlp_input(paths, h)\n",
    "\n",
    "            # run the feedforward and get the scores for each possible action\n",
    "            # returns 3 float numbers \n",
    "        out = self.mlp(mlp_input)\n",
    "\n",
    "        return out \n",
    "        \n",
    "\n",
    "    def lstm_pass(self, x):\n",
    "        x = torch.nn.utils.rnn.pack_sequence(x, enforce_sorted = False)\n",
    "        h, (h_0, c_0) = self.lstm(x)\n",
    "        h, h_sizes = torch.nn.utils.rnn.pad_packed_sequence(h)\n",
    "        return h\n",
    "        \n",
    "\n",
    "    def get_mlp_input(self, configurations, h):\n",
    "        mlp_input = []\n",
    "        zero_tensor = torch.zeros(2*LSTM_SIZE, requires_grad=False).to(self.device)\n",
    "        for i in range(len(configurations)):\n",
    "            for j in configurations[i]:\n",
    "                mlp_input.append(torch.cat([zero_tensor if j[0]==-1 else h[j[0]][i], zero_tensor if j[1]==-1 else h[j[1]][i], zero_tensor if j[2]==-1 else h[j[2]][i]]))\n",
    "        mlp_input = torch.stack(mlp_input).to(self.device)\n",
    "            \n",
    "        return mlp_input\n",
    "        \n",
    "    def mlp(self, x):\n",
    "        return self.softmax(self.w2(self.dropout(self.activation(self.w1(self.dropout(x))))))\n",
    "        \n",
    "\n",
    "        # we use this function at inference time. We run the parser and at each step \n",
    "        # we pick as next move the one with the highest score assigned by  the model\n",
    "        # do a move, get a parser configuration and predict the next\n",
    "\n",
    "    def infere(self, x):\n",
    "\n",
    "        parsers = [ArcStandard(i) for i in x]\n",
    "\n",
    "        x = [self.embeddings(torch.tensor(i).to(self.device)) for i in x]\n",
    "            \n",
    "        h = self.lstm_pass(x)\n",
    "\n",
    "        while not self.parsed_all(parsers):\n",
    "\n",
    "                # get the current configuration and score next move\n",
    "\n",
    "            configurations = self.get_configurations(parsers)\n",
    "            mlp_input = self.get_mlp_input(configurations, h)\n",
    "            mlp_out = self.mlp(mlp_input)\n",
    "\n",
    "                # take the next parsing step\n",
    "                # this method will chose which one of the moves to do next; \n",
    "                # here also we add contraints in order to not take wrong actions\n",
    "\n",
    "            self.parse_step(parsers, mlp_out)\n",
    "\n",
    "                # return the predicted dependency tree\n",
    "\n",
    "        return [parser.arcs for parser in parsers]\n",
    "            \n",
    "    def get_configurations(self, parsers):\n",
    "        configurations = []\n",
    "\n",
    "        for parser in parsers:\n",
    "            if parser.is_tree_final():\n",
    "                conf = [-1, -1, -1]\n",
    "            else:\n",
    "                conf = [parser.stack[len(parser.stack)-2], parser.stack[len(parser.stack)-1]]\n",
    "                if len(parser.buffer) == 0:\n",
    "                        conf.append(-1)\n",
    "                else:\n",
    "                    conf.append(parser.buffer[0])\n",
    "            configurations.append([conf])\n",
    "        return configurations\n",
    "\n",
    "    def parsed_all(self, parsers):\n",
    "        for parser in parsers:\n",
    "            if not parser.is_tree_final():\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "              # In this function we select and perform the next move according to the scores obtained.\n",
    "              # We need to be careful and select correct moves, e.g. don't do a shift if the buffer\n",
    "              # is empty or a left arc if σ2 is the ROOT. For clarity sake we didn't implement\n",
    "              # these checks in the parser so we must do them here. This renders the function quite ugly\n",
    "\n",
    "    def parse_step(self, parsers, moves):\n",
    "\n",
    "        moves_argm = moves.argmax(-1)\n",
    "        for i in range(len(parsers)):\n",
    "            if parsers[i].is_tree_final():\n",
    "                continue\n",
    "            else:\n",
    "                if moves_argm[i] == 0:\n",
    "                    if parsers[i].stack[len(parsers[i].stack)-2] != 0:\n",
    "                        parsers[i].left_arc()\n",
    "                    else:\n",
    "                        if len(parsers[i].buffer) > 0:\n",
    "                            parsers[i].shift()\n",
    "                        else:\n",
    "                            parsers[i].right_arc()\n",
    "                elif moves_argm[i] ==1:\n",
    "                    if parsers[i].stack[len(parsers[i].stack)-2] ==0 and len(parsers[i].buffer) >0:\n",
    "                        parsers[i].shift()\n",
    "                    else:\n",
    "                        parsers[i].right_arc()\n",
    "                elif moves_argm[i] == 2:\n",
    "                    if len(parsers[i].buffer) > 0:\n",
    "                        parsers[i].shift()\n",
    "                    else:\n",
    "                        if moves[i][0] > moves[i][1]:\n",
    "                            if parsers[i].stack[len(parsers[i].stack)-2] != 0:\n",
    "\n",
    "                                parsers[i].left_arc()\n",
    "                            else:\n",
    "                                parsers[i].right_arc()\n",
    "                        else:\n",
    "                            parsers[i].right_arc()\n",
    "\n",
    "                                    \n",
    "                \n",
    "\n",
    "\"\"\"                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "  def __init__(self, device):\n",
    "    super(Net, self).__init__()\n",
    "    self.device = device\n",
    "    self.embeddings = nn.Embedding(len(emb_dictionary), EMBEDDING_SIZE, padding_idx=emb_dictionary[\"<pad>\"])\n",
    "    \n",
    "    # initialize bi-LSTM\n",
    "    self.lstm = nn.LSTM(EMBEDDING_SIZE, LSTM_SIZE, num_layers = LSTM_LAYERS, bidirectional=True, dropout=DROPOUT)\n",
    "\n",
    "    # initialize feedforward\n",
    "    self.w1 = torch.nn.Linear(6*LSTM_SIZE, MLP_SIZE, bias=True)\n",
    "    self.activation = torch.nn.Tanh()\n",
    "    self.w2 = torch.nn.Linear(MLP_SIZE, 3, bias=True)\n",
    "    self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "  \n",
    "  \n",
    "  def forward(self, x, paths):\n",
    "    # get the embeddings \n",
    "    x = [self.dropout(self.embeddings(torch.tensor(i).to(self.device))) for i in x]\n",
    "\n",
    "    # run the bi-lstm\n",
    "    h = self.lstm_pass(x)\n",
    "\n",
    "    # for each parser configuration that we need to score we arrange from the\n",
    "    # output of the bi-lstm the correct input for the feedforward\n",
    "    mlp_input = self.get_mlp_input(paths, h)\n",
    "\n",
    "    # run the feedforward and get the scores for each possible action\n",
    "    out = self.mlp(mlp_input)\n",
    "\n",
    "    return out\n",
    "\n",
    "  def lstm_pass(self, x):\n",
    "    x = torch.nn.utils.rnn.pack_sequence(x, enforce_sorted=False)\n",
    "    h, (h_0, c_0) = self.lstm(x)\n",
    "    h, h_sizes = torch.nn.utils.rnn.pad_packed_sequence(h) # size h: (length_sentences, batch, output_hidden_units)\n",
    "    return h\n",
    "\n",
    "  def get_mlp_input(self, configurations, h):\n",
    "    mlp_input = []\n",
    "    zero_tensor = torch.zeros(2*LSTM_SIZE, requires_grad=False).to(self.device)\n",
    "    for i in range(len(configurations)): # for every sentence in the batch\n",
    "      for j in configurations[i]: # for each configuration of a sentence \n",
    "        mlp_input.append(torch.cat([zero_tensor if j[0]==-1 else h[j[0]][i], zero_tensor if j[1]==-1 else h[j[1]][i], zero_tensor if j[2]==-1 else h[j[2]][i]]))\n",
    "    mlp_input = torch.stack(mlp_input).to(self.device)\n",
    "    return mlp_input\n",
    "\n",
    "  def mlp(self, x):\n",
    "    return self.softmax(self.w2(self.dropout(self.activation(self.w1(self.dropout(x))))))\n",
    "\n",
    "  # we use this function at inference time. We run the parser and at each step \n",
    "  # we pick as next move the one with the highest score assigned by the model\n",
    "  def infere(self, x):\n",
    "\n",
    "    parsers = [ArcStandard(i) for i in x]\n",
    "\n",
    "    x = [self.embeddings(torch.tensor(i).to(self.device)) for i in x]\n",
    "\n",
    "    h = self.lstm_pass(x)\n",
    "\n",
    "    while not self.parsed_all(parsers):\n",
    "      # get the current configuration and score next moves\n",
    "      configurations = self.get_configurations(parsers)\n",
    "      mlp_input = self.get_mlp_input(configurations, h)\n",
    "      mlp_out = self.mlp(mlp_input)\n",
    "      # take the next parsing step\n",
    "      self.parse_step(parsers, mlp_out)\n",
    "\n",
    "    # return the predicted dependency tree\n",
    "    return [parser.arcs for parser in parsers]\n",
    "\n",
    "  def get_configurations(self, parsers):\n",
    "    configurations = []\n",
    "\n",
    "    for parser in parsers:\n",
    "      if parser.is_tree_final():\n",
    "        conf = [-1, -1, -1]\n",
    "      else:\n",
    "        conf = [parser.stack[len(parser.stack)-2], parser.stack[len(parser.stack)-1]]\n",
    "        if len(parser.buffer) == 0:\n",
    "          conf.append(-1)\n",
    "        else:\n",
    "          conf.append(parser.buffer[0])  \n",
    "      configurations.append([conf])\n",
    "\n",
    "    return configurations\n",
    "\n",
    "  def parsed_all(self, parsers):\n",
    "    for parser in parsers:\n",
    "      if not parser.is_tree_final():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "  # In this function we select and perform the next move according to the scores obtained.\n",
    "  # We need to be careful and select correct moves, e.g. don't do a shift if the buffer\n",
    "  # is empty or a left arc if σ2 is the ROOT. For clarity sake we didn't implement\n",
    "  # these checks in the parser so we must do them here. This renders the function quite ugly\n",
    "  def parse_step(self, parsers, moves):\n",
    "    moves_argm = moves.argmax(-1)\n",
    "    for i in range(len(parsers)):\n",
    "      if parsers[i].is_tree_final():\n",
    "        continue\n",
    "      else:\n",
    "        if moves_argm[i] == 0:\n",
    "          if parsers[i].stack[len(parsers[i].stack)-2] != 0:\n",
    "            parsers[i].left_arc()\n",
    "          else:\n",
    "            if len(parsers[i].buffer) > 0:\n",
    "              parsers[i].shift()\n",
    "            else:\n",
    "              parsers[i].right_arc()\n",
    "        elif moves_argm[i] == 1:\n",
    "          if parsers[i].stack[len(parsers[i].stack)-2] == 0 and len(parsers[i].buffer)>0:\n",
    "            parsers[i].shift()\n",
    "          else:\n",
    "            parsers[i].right_arc()\n",
    "        elif moves_argm[i] == 2:\n",
    "          if len(parsers[i].buffer) > 0:\n",
    "            parsers[i].shift()\n",
    "          else:\n",
    "            if moves[i][0] > moves[i][1]:\n",
    "              if parsers[i].stack[len(parsers[i].stack)-2] != 0:\n",
    "                parsers[i].left_arc()\n",
    "              else:\n",
    "                parsers[i].right_arc()\n",
    "            else:\n",
    "              parsers[i].right_arc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold, preds): \n",
    "  total = 0\n",
    "  correct = 0\n",
    "\n",
    "  for g, p in zip(gold, preds):\n",
    "    for i in range(1,len(g)):\n",
    "      total += 1\n",
    "      if g[i] == p[i]:\n",
    "        correct += 1\n",
    "\n",
    "  return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  count = 0\n",
    "\n",
    "  for batch in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    sentences, paths, moves, trees = batch\n",
    "\n",
    "    out = model(sentences, paths)\n",
    "    labels = torch.tensor(sum(moves, [])).to(device) #sum(moves, []) flatten the array\n",
    "    loss = criterion(out, labels)\n",
    "\n",
    "    count +=1\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  return total_loss/count\n",
    "\n",
    "def test(model, dataloader):\n",
    "  model.eval()\n",
    "\n",
    "  gold = []\n",
    "  preds = []\n",
    "\n",
    "  for batch in dataloader:\n",
    "    sentences, paths, moves, trees = batch\n",
    "    with torch.no_grad():\n",
    "      pred = model.infere(sentences)\n",
    "\n",
    "      gold += trees\n",
    "      preds += pred\n",
    "  \n",
    "  return evaluate(gold, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Epoch:   0 | avg_train_loss: 0.828 | dev_uas: 0.590 |\n",
      "Epoch:   1 | avg_train_loss: 0.738 | dev_uas: 0.646 |\n",
      "Epoch:   2 | avg_train_loss: 0.716 | dev_uas: 0.672 |\n",
      "Epoch:   3 | avg_train_loss: 0.704 | dev_uas: 0.685 |\n",
      "Epoch:   4 | avg_train_loss: 0.693 | dev_uas: 0.693 |\n",
      "Epoch:   5 | avg_train_loss: 0.685 | dev_uas: 0.703 |\n",
      "Epoch:   6 | avg_train_loss: 0.678 | dev_uas: 0.709 |\n",
      "Epoch:   7 | avg_train_loss: 0.673 | dev_uas: 0.708 |\n",
      "Epoch:   8 | avg_train_loss: 0.669 | dev_uas: 0.715 |\n",
      "Epoch:   9 | avg_train_loss: 0.665 | dev_uas: 0.719 |\n",
      "Epoch:  10 | avg_train_loss: 0.662 | dev_uas: 0.718 |\n",
      "Epoch:  11 | avg_train_loss: 0.658 | dev_uas: 0.722 |\n",
      "Epoch:  12 | avg_train_loss: 0.653 | dev_uas: 0.730 |\n",
      "Epoch:  13 | avg_train_loss: 0.649 | dev_uas: 0.731 |\n",
      "Epoch:  14 | avg_train_loss: 0.646 | dev_uas: 0.733 |\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "model = Net(device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  avg_train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "  val_uas = test(model, dev_dataloader)\n",
    "\n",
    "  print(\"Epoch: {:3d} | avg_train_loss: {:5.3f} | dev_uas: {:5.3f} |\".format( epoch, avg_train_loss, val_uas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_uas: 0.736\n"
     ]
    }
   ],
   "source": [
    "test_uas = test(model, test_dataloader)\n",
    "print(\"test_uas: {:5.3f}\".format(test_uas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
